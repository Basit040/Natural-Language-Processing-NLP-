{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "689bc3cc",
   "metadata": {},
   "source": [
    "# EXPANDING NAME ENTITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13275849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d598fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae77d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= nlp('Dr. Alex Smith chaired first board meeting at Google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e2e9ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr. Alex Smith chaired first board meeting at Google"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a2435e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alex Smith', 'PERSON'), ('first', 'ORDINAL'), ('Google', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3730c273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy[transformers] in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (3.2.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (2.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (58.0.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (1.0.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (8.0.13)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (1.20.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (0.7.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (0.6.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (4.62.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (2.26.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (1.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (21.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (2.11.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from spacy[transformers]) (0.4.0)\n",
      "Collecting spacy-transformers<1.2.0,>=1.1.2\n",
      "  Downloading spacy_transformers-1.1.4-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy[transformers]) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy[transformers]) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy[transformers]) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (1.26.7)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.11.0-cp39-cp39-win_amd64.whl (157.9 MB)\n",
      "Collecting transformers<4.16.0,>=3.4.0\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
      "  Downloading spacy_alignments-0.8.4-cp39-cp39-win_amd64.whl (183 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy[transformers]) (0.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from transformers<4.16.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (3.3.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from transformers<4.16.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (2021.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from transformers<4.16.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (6.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy[transformers]) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from jinja2->spacy[transformers]) (1.1.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from sacremoses->transformers<4.16.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (1.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\abdul basit\\anaconda3\\lib\\site-packages (from sacremoses->transformers<4.16.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[transformers]) (1.16.0)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers, torch, spacy-alignments, spacy-transformers\n",
      "Successfully installed huggingface-hub-0.4.0 sacremoses-0.0.47 spacy-alignments-0.8.4 spacy-transformers-1.1.4 tokenizers-0.10.3 torch-1.11.0 transformers-4.15.0\n"
     ]
    }
   ],
   "source": [
    "# Installing spacy transformer\n",
    "!pip install spacy[transformers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441394da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577c978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d8226fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "975b0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing function to extract DR Mr Ms etc\n",
    "@Language.component(\"add_title\")\n",
    "def add_title(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and ent.start != 0:\n",
    "            prev_token = doc[ent.start - 1]\n",
    "            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
    "                new_ents.append(new_ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d8d6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dr. Alex Smith', 'PERSON'), ('first', 'ORDINAL'), ('Acme Corp Inc.', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# Add the component after the named entity recognizer\n",
    "nlp.add_pipe(\"add_title\", after=\"ner\")\n",
    "\n",
    "doc = nlp(\"Dr. Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51eb75a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E090] Extension 'person_title' already exists on Span. To overwrite the existing extension, set `force=True` on `Span.set_extension`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ABDULB~1\\AppData\\Local\\Temp/ipykernel_9564/638369030.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Register the Span extension as 'person_title'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mSpan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"person_title\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_person_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\tokens\\span.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.set_extension\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E090] Extension 'person_title' already exists on Span. To overwrite the existing extension, set `force=True` on `Span.set_extension`."
     ]
    }
   ],
   "source": [
    "# Another approach to do ectract Dr Mr etc\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_person_title(span):\n",
    "    if span.label_ == \"PERSON\" and span.start != 0:\n",
    "        prev_token = span.doc[span.start - 1]\n",
    "        if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "            return prev_token.text\n",
    "\n",
    "# Register the Span extension as 'person_title'\n",
    "Span.set_extension(\"person_title\", getter=get_person_title)\n",
    "\n",
    "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_, ent._.person_title) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d75441",
   "metadata": {},
   "source": [
    "# USE OF POS AND DEEP PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7740efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec7afd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc= nlp(\"Alex Smith was working at Google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee6b5d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9f43eb91def84e3699ac7b6bc91cb7cc-0\" class=\"displacy\" width=\"650\" height=\"237.0\" direction=\"ltr\" style=\"max-width: none; height: 237.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Alex</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">Smith</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">working</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">Google</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9f43eb91def84e3699ac7b6bc91cb7cc-0-0\" stroke-width=\"2px\" d=\"M62,102.0 62,85.33333333333333 147.0,85.33333333333333 147.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9f43eb91def84e3699ac7b6bc91cb7cc-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,104.0 L58,96.0 66,96.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9f43eb91def84e3699ac7b6bc91cb7cc-0-1\" stroke-width=\"2px\" d=\"M162,102.0 162,68.66666666666666 350.0,68.66666666666666 350.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9f43eb91def84e3699ac7b6bc91cb7cc-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M162,104.0 L158,96.0 166,96.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9f43eb91def84e3699ac7b6bc91cb7cc-0-2\" stroke-width=\"2px\" d=\"M262,102.0 262,85.33333333333333 347.0,85.33333333333333 347.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9f43eb91def84e3699ac7b6bc91cb7cc-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M262,104.0 L258,96.0 266,96.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9f43eb91def84e3699ac7b6bc91cb7cc-0-3\" stroke-width=\"2px\" d=\"M362,102.0 362,85.33333333333333 447.0,85.33333333333333 447.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9f43eb91def84e3699ac7b6bc91cb7cc-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M447.0,104.0 L451.0,96.0 443.0,96.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-9f43eb91def84e3699ac7b6bc91cb7cc-0-4\" stroke-width=\"2px\" d=\"M462,102.0 462,85.33333333333333 547.0,85.33333333333333 547.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-9f43eb91def84e3699ac7b6bc91cb7cc-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M547.0,104.0 L551.0,96.0 543.0,96.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style='dep', options={'compact':True, 'distance':100})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b6ea504",
   "metadata": {},
   "source": [
    "In this example, “worked” is the root of the sentence and is a past tense verb. Its subject is “Alex Smith”, the person who worked. “at Acme Corp Inc.” is a prepositional phrase attached to the verb “worked”. To extract this relationship, we can start by looking at the predicted PERSON entities, find their heads and check whether they’re attached to a trigger word like “work”. Next, we can check for prepositional phrases attached to the head and whether they contain an ORG entity. Finally, to determine whether the company affiliation is current, we can check the head’s part-of-speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b00bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"get_person_orgs\")\n",
    "def get_person_orgs(doc):\n",
    "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for ent in person_entities:\n",
    "        # Because the entity is a span, we need to use its root token. The head\n",
    "        # is the syntactic governor of the person, e.g. the verb\n",
    "        head = ent.root.head\n",
    "        if head.lemma_ == \"work\":\n",
    "            # Check if the children contain a preposition\n",
    "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "            for prep in preps:\n",
    "            # Check if tokens part of ORG entities are in the preposition's\n",
    "            # children, e.g. at -> Acme Corp Inc.\n",
    "                orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
    "            # If the verb is in past tense, the company was a previous company\n",
    "                print({\"person\": ent, \"orgs\": orgs, \"past\": head.tag_ == \"VBD\"})\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbdd273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52f22a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to extract whether it is a past or oresent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a026d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import merge_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0574061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4845d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69068244",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E007] 'merge_entities' already exists in pipeline. Existing names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner', 'merge_entities']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ABDULB~1\\AppData\\Local\\Temp/ipykernel_9564/961587196.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# To make the entities easier to work with, we'll merge them into single tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"merge_entities\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#nlp.add_pipe(\"get_person_orgs\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# We're loading the component from a model. After loading the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E007] 'merge_entities' already exists in pipeline. Existing names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner', 'merge_entities']"
     ]
    }
   ],
   "source": [
    "# To make the entities easier to work with, we'll merge them into single tokens\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "#nlp.add_pipe(\"get_person_orgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e49956a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_person_orgs(doc)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"get_person_orgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47db1171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': Alex Smith, 'orgs': [Google], 'past': False}\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Alex Smith was working at Google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64b70cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': Alex Smith, 'orgs': [Google], 'past': True}\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Alex Smith worked at Google\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e65e65b",
   "metadata": {},
   "source": [
    "f you change the sentence structure above, for example to “was working”, you’ll notice that our current logic fails and doesn’t correctly detect the company as a past organization. That’s because the root is a participle and the tense information is in the attached auxiliary “was”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf49f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify model so it will be working for \"was\" as well, we will change the function name as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180f04c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58fe87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"extract_person_orgs\")\n",
    "def extract_person_orgs(doc):\n",
    "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for ent in person_entities:\n",
    "        head = ent.root.head\n",
    "        if head.lemma_ == \"work\":\n",
    "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "            for prep in preps:\n",
    "                orgs = [t for t in prep.children if t.ent_type_ == \"ORG\"]\n",
    "                aux = [token for token in head.children if token.dep_ == \"aux\"]\n",
    "                past_aux = any(t.tag_ == \"VBD\" for t in aux)\n",
    "                past = head.tag_ == \"VBD\" or head.tag_ == \"VBG\" and past_aux\n",
    "                print({'person': ent, 'orgs': orgs, 'past': past})\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf0b167b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extract_person_orgs(doc)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"extract_person_orgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b34d3995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': Alex Smith, 'orgs': [Google], 'past': False}\n",
      "{'person': Alex Smith, 'orgs': [Google], 'past': True}\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Alex Smith was working at Google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80b42b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': Alex Smith, 'orgs': [Google], 'past': True}\n",
      "{'person': Alex Smith, 'orgs': [Google], 'past': True}\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Alex Smith worked at Google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c6ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fe74a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
